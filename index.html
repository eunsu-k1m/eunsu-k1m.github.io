<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Eunsu Kim</title>

    <meta name="author" content="Jon Barron">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="img/favicon/favicon-16x16.png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Eunsu Kim
                </p>
                <p class="bigger">
                    I am a master's student advised by Professor <a href="https://aliceoh9.github.io/">Alice Oh</a> at 
                    <a href="https://cs.kaist.ac.kr/" class="bigger">School of Computing, KAIST</a>. <br><br>
                    My current research focuses on the evaluation of LLMs, specifically: <br>
                    
                    (1) What to evaluate: Exploring the direction in which LLMs should progress and examining current LLM behavior from that perspective. (2) How to evaluate: Developing evaluation frameworks/metrics that measure the true capabilities of LLMs. 3) Interesting behaviors during evaluation: Investigating the unique and unexpected behaviors observed during the evaluations in (1) and (2). <br>
                    I believe accurate evaluation in the proper context can guide LLMs to evolve in meaningful and appropriate directions.

                    <br><br>
                    Currently, I'm working on developing an LLM evaluation framework that is reliable and interpretable, and benchmarking cultural awareness of (V)LM in interesting scenarios.
                    <br><br>
                    If you would like to collaborate with me or have any questions, Feel free to contact me!
                </p>

                
                <div style="text-align: center;">
                    <a href="kes0317@kaist.ac.kr">Email</a> &nbsp;/&nbsp;
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=eoL3C_MAAAAJ&hl=ko">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/euns0o-kim/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://x.com/euns0o_kim">Twitter(X)</a> &nbsp;/&nbsp;
                    <a href="https://eunsu-k1m.github.io/cv_eunsu_240519.pdf">CV</a>
                    <!-- <a href="https://github.com/jonbarron/">Github</a> -->
                </div>

              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <div  class="image-container">
                <img src="img/photo.jpg" alt="profile photo" class=default-img"></a>
                <img src="img/photo2.jpg" alt="hover profile photo" class="hover-img">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Education</h2>
                <p>
                  <div class="education-entry">
                    <h3>Korea Advanced Institute of Science and Technology (KAIST)</h3>
                    <p>
                      <span class="container">
                        <em class="left">M.S. in Computer Science, Advisor: Alice Oh</em> 
                        <span class="right">2023.09-present</span>
                      </span>
                    </p>
                    <p>
                       <span class="container">
                        <em class="left">B.S. in Electrical Engineering</em> 
                        <span class="right"> 2019.03-2023.08</span>
                      </span>   
                      <span class="smallicon">&#9679;</span> <em><strong>GPA: 4.02/4.3, Major GPA: 4.15/4.3 (Summa Cum Laude)</strong></em>
                    </p>
                    
                  </div>
                  
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <h3>* denotes equal contributions</h3>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <!--   /ICLR 2025: PROFILE  -->
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:100%;vertical-align:middle">
              <strong>
                <span class="papertitle">Discovering Factor Level Preferences to Improve Human-Model Alignment</span>
              </strong>
              <br>
              Juhyun Oh*, <strong><u>Eunsu Kim*</u></strong>, Jiseon Kim, Wenda Xu, William Yang Wang, Alice Oh
              <br>
              <em>Under Reviewed
              <br>
              </td>
          </tr>
          <!--    /ICLR 2025: PROFILE   -->                
            
          <!--   /NIPS 2024: BLEND  -->
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:100%;vertical-align:middle">
              <strong>
                <span class="papertitle">BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages</span>
              </strong>
              <br>
          Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty,<strong><u>Eunsu Kim</u></strong>, Carla Perez-Almendros, Abinew Ali Ayele, Víctor Gutiérrez-Basulto, Yazmín Ibáñez-García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh
              <br>
              <em>Neurips D&B, 2025
              <br>
              <a href="https://arxiv.org/abs/2406.09948">arXiv</a>
              <a href="https://huggingface.co/datasets/nayeon212/BLEnD">Dataset</a>
              <a onclick="toggleText('hidden-text-blend')">TL;DR</a>
              <p id="hidden-text-blend" class="hidden-text">
              Large language models (LLMs) often lack culture-specific knowledge of daily life, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are limited to a single language or collected from online sources such as Wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We construct the benchmark to include two formats of questions: short-answer and multiple-choice. We show that LLMs perform better for cultures that are highly represented online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format. For cultures represented by mid-to-high-resource languages, LLMs perform better in their local languages, but for cultures represented by low-resource languages, LLMs perform better in English than the local languages.               </p>
            </td>
          </tr>
          <!--   /NIPS 2024: BLEND  -->    


          <!--   /LREC-COLING 2024: CLIcK  -->
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:100%;vertical-align:middle">
              <strong>
                <span class="papertitle">CLIcK: Evaluation of Cultural and Linguistic Intelligence in Korean</span>
              </strong>
              <br>
          <strong><u>Eunsu Kim</u></strong>,
          Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice Oh
              <br>
              <em>LREC-COLING 2024
              <br>
              <a href="https://arxiv.org/abs/2403.06412">arXiv</a>
              <a href="https://github.com/rladmstn1714/CLIcK">Dataset</a>
              <a onclick="toggleText('hidden-text-click')">TL;DR</a>
              <p id="hidden-text-click" class="hidden-text">
               <img src='img/CLICK.png' width=30%><br>
              We construct and release CLIcK, a culturally-aware evaluation benchmark dataset encompassing 1,995 instances across 11 categories representing facets of the Korean culture, ranging from everyday life to specific subject areas, as well as Korean grammar and linguistics.
              </p>
            </td>
          </tr>
          <!--   /LREC-COLING 2024: CLIcK  -->      
            
          <!--   /ICLR 2024 : multi-fact  -->
            
          <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
              <td style="padding:20px;width:100%;vertical-align:middle">
              <strong>
                <span class="papertitle">Multi-FAct: Assessing Multilingual LLMs' Multi-Regional Knowledge using FActScore</span>
              </strong>
              <br>
          Sheikh Shafayat, <strong><u>Eunsu Kim*</u></strong>, Juhyun Oh*, Alice Oh
              <br>
              <em>COLM 2024, Workshop on Global AI Cultures at ICLR 2024
              <br>
              <a href="https://arxiv.org/abs/2402.18045">arXiv</a>
              <a onclick="toggleText('hidden-text-mfact')">TL;DR</a>
              <p id="hidden-text-mfact" class="hidden-text">
               <img src='img/multi-fact.png' width=80%><br>
                We introduce a novel pipeline tailored for evaluating factuality in a multilingual setting. Our approach first adapts the FActScore (Min et al., 2023) to accommodate multiple languages, and we make this pipeline openly accessible as open-source.
              </p>
            </td>
          </tr>
          <!--   /ICLR 2024 : multi-fact  -->

          <!--   /EACL 2024 : paradox  -->
        <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
            <td style="padding:20px;width:100%;vertical-align:middle">
                <strong>
                    <span class="papertitle">The Generative AI Paradox in Evaluation: "What It Can Solve, It May Not Evaluate</span>
                </strong>
                <br>
                Juhyun Oh*, <strong><u>Eunsu Kim*</u></strong>, Inha Cha*, Alice Oh
                <br>
                <em>EACL SRW 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2402.06204">arXiv</a>
                <a onclick="toggleText('hidden-text-para')">TL;DR</a>
                <p id="hidden-text-para" class="hidden-text">
                  <img src='img/paradox.png' width=80%><br>
                    This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset.
                </p>
            </td>
        </tr>

          <!--   /EACL 2024 : paradox  -->

          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template from <a href=" https://jonbarron.info/">Jon Barron's</a> wonderful work.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
      <script>
          function toggleText(id) {
              const textElement = document.getElementById(id);
              if (textElement.style.display === 'none' || textElement.style.display === '') {
                  textElement.style.display = 'block';
              } else {
                  textElement.style.display = 'none';
              }
          }
      </script>
  </body>
</html>
